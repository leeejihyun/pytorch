{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functioning-journey",
   "metadata": {},
   "source": [
    "# Lab 7-1: Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blond-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dressed-linux",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c979fbfeb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproductibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-friendly",
   "metadata": {},
   "source": [
    "## Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frequent-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1, 2, 1],\n",
    "                            [1, 3, 2],\n",
    "                            [1, 3, 4],\n",
    "                            [1, 5, 5],\n",
    "                            [1, 7, 5],\n",
    "                            [1, 2, 5],\n",
    "                            [1, 6, 6],\n",
    "                            [1, 7, 7]])\n",
    "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adapted-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\n",
    "y_test = torch.LongTensor([2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-music",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "substantial-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "difficult-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "normal-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "referenced-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost 계산\n",
    "        cost = F.cross_entropy(prediction, y_train)\n",
    "        \n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "global-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, optimizer, x_test, y_test):\n",
    "    prediction = model(x_test)\n",
    "#     print(\"prediction:\", prediction)\n",
    "#     print(\"prediction.max(1):\", prediction.max(1))\n",
    "    predicted_classes = prediction.max(1)[1] # dim=1에 대해 max, 그리고 argmax 가져오기\n",
    "#     print(\"prediction.max(1)[1]:\", predicted_classes)\n",
    "    correct_count = (predicted_classes == y_test).sum().item()\n",
    "#     print(\"before .item():\", (predicted_classes == y_test).sum())\n",
    "#     print(\"after .item():\", (predicted_classes == y_test).sum().item()) # cost.item()처럼 값 가져오기\n",
    "    cost = F.cross_entropy(prediction, y_test)\n",
    "    \n",
    "    print('Accuracy: {}% Cost {:.6f}'.format(\n",
    "    correct_count / len(y_test) * 100, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stable-finnish",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 2.203667\n",
      "Epoch    1/20 Cost: 1.199645\n",
      "Epoch    2/20 Cost: 1.142985\n",
      "Epoch    3/20 Cost: 1.117769\n",
      "Epoch    4/20 Cost: 1.100901\n",
      "Epoch    5/20 Cost: 1.089523\n",
      "Epoch    6/20 Cost: 1.079872\n",
      "Epoch    7/20 Cost: 1.071320\n",
      "Epoch    8/20 Cost: 1.063325\n",
      "Epoch    9/20 Cost: 1.055720\n",
      "Epoch   10/20 Cost: 1.048377\n",
      "Epoch   11/20 Cost: 1.041245\n",
      "Epoch   12/20 Cost: 1.034285\n",
      "Epoch   13/20 Cost: 1.027478\n",
      "Epoch   14/20 Cost: 1.020813\n",
      "Epoch   15/20 Cost: 1.014279\n",
      "Epoch   16/20 Cost: 1.007872\n",
      "Epoch   17/20 Cost: 1.001586\n",
      "Epoch   18/20 Cost: 0.995419\n",
      "Epoch   19/20 Cost: 0.989365\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "soviet-richards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0% Cost 1.425844\n"
     ]
    }
   ],
   "source": [
    "test(model, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-hygiene",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-leather",
   "metadata": {},
   "source": [
    "learning rate가 너무 크면 발산하면서 cost가 점점 늘어난다. (overshooting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baking-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secure-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "active-charleston",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 1.280268\n",
      "Epoch    1/20 Cost: 976950.625000\n",
      "Epoch    2/20 Cost: 1279135.125000\n",
      "Epoch    3/20 Cost: 1198379.250000\n",
      "Epoch    4/20 Cost: 1098825.625000\n",
      "Epoch    5/20 Cost: 1968197.625000\n",
      "Epoch    6/20 Cost: 284763.125000\n",
      "Epoch    7/20 Cost: 1532260.000000\n",
      "Epoch    8/20 Cost: 1651504.250000\n",
      "Epoch    9/20 Cost: 521878.437500\n",
      "Epoch   10/20 Cost: 1397263.125000\n",
      "Epoch   11/20 Cost: 750986.250000\n",
      "Epoch   12/20 Cost: 918691.750000\n",
      "Epoch   13/20 Cost: 1487888.125000\n",
      "Epoch   14/20 Cost: 1582260.125000\n",
      "Epoch   15/20 Cost: 685818.000000\n",
      "Epoch   16/20 Cost: 1140048.750000\n",
      "Epoch   17/20 Cost: 940566.750000\n",
      "Epoch   18/20 Cost: 931638.125000\n",
      "Epoch   19/20 Cost: 1971322.625000\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-ukraine",
   "metadata": {},
   "source": [
    "learning rate가 너무 작으면 cost가 거의 줄어들지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continuing-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "herbal-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "promotional-relief",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 3.187324\n",
      "Epoch    1/20 Cost: 3.187324\n",
      "Epoch    2/20 Cost: 3.187324\n",
      "Epoch    3/20 Cost: 3.187324\n",
      "Epoch    4/20 Cost: 3.187324\n",
      "Epoch    5/20 Cost: 3.187324\n",
      "Epoch    6/20 Cost: 3.187324\n",
      "Epoch    7/20 Cost: 3.187324\n",
      "Epoch    8/20 Cost: 3.187324\n",
      "Epoch    9/20 Cost: 3.187324\n",
      "Epoch   10/20 Cost: 3.187324\n",
      "Epoch   11/20 Cost: 3.187324\n",
      "Epoch   12/20 Cost: 3.187324\n",
      "Epoch   13/20 Cost: 3.187324\n",
      "Epoch   14/20 Cost: 3.187324\n",
      "Epoch   15/20 Cost: 3.187324\n",
      "Epoch   16/20 Cost: 3.187324\n",
      "Epoch   17/20 Cost: 3.187324\n",
      "Epoch   18/20 Cost: 3.187324\n",
      "Epoch   19/20 Cost: 3.187324\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-appendix",
   "metadata": {},
   "source": [
    "따라서 적절한 숫자로 시작해 발산하면 작게, cost가 줄어들지 않으면 크게 조정하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "peripheral-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unlikely-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "systematic-designer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 1.341574\n",
      "Epoch    1/20 Cost: 1.198802\n",
      "Epoch    2/20 Cost: 1.150877\n",
      "Epoch    3/20 Cost: 1.131977\n",
      "Epoch    4/20 Cost: 1.116242\n",
      "Epoch    5/20 Cost: 1.102514\n",
      "Epoch    6/20 Cost: 1.089676\n",
      "Epoch    7/20 Cost: 1.077479\n",
      "Epoch    8/20 Cost: 1.065775\n",
      "Epoch    9/20 Cost: 1.054511\n",
      "Epoch   10/20 Cost: 1.043655\n",
      "Epoch   11/20 Cost: 1.033187\n",
      "Epoch   12/20 Cost: 1.023091\n",
      "Epoch   13/20 Cost: 1.013356\n",
      "Epoch   14/20 Cost: 1.003968\n",
      "Epoch   15/20 Cost: 0.994917\n",
      "Epoch   16/20 Cost: 0.986189\n",
      "Epoch   17/20 Cost: 0.977775\n",
      "Epoch   18/20 Cost: 0.969661\n",
      "Epoch   19/20 Cost: 0.961836\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-candidate",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-humidity",
   "metadata": {},
   "source": [
    "데이터를 zero-center하고 normalize하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "executive-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "based-infrared",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([84.8000, 84.6000, 85.6000])\n"
     ]
    }
   ],
   "source": [
    "mu = x_train.mean(dim=0)\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "representative-procedure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.0544, 12.2393, 12.6214])\n"
     ]
    }
   ],
   "source": [
    "sigma = x_train.std(dim=0)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "received-telescope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0674, -0.3758, -0.8398],\n",
      "        [ 0.7418,  0.2778,  0.5863],\n",
      "        [ 0.3799,  0.5229,  0.3486],\n",
      "        [ 1.0132,  1.0948,  1.1409],\n",
      "        [-1.0674, -1.5197, -1.2360]])\n"
     ]
    }
   ],
   "source": [
    "norm_x_train = (x_train - mu) / sigma\n",
    "print(norm_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-principle",
   "metadata": {},
   "source": [
    "normalize와 zero center한 X로 학습해서 성능을 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "answering-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "imposed-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nervous-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fleet-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "stuck-worry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 57621.960938\n",
      "Epoch    1/20 Cost: 1115690762240.000000\n",
      "Epoch    2/20 Cost: 21603375986356781056.000000\n",
      "Epoch    3/20 Cost: 418311133762336799454134272.000000\n",
      "Epoch    4/20 Cost: 8099857151294734205918406036684800.000000\n",
      "Epoch    5/20 Cost:    inf\n",
      "Epoch    6/20 Cost:    inf\n",
      "Epoch    7/20 Cost:    inf\n",
      "Epoch    8/20 Cost:    inf\n",
      "Epoch    9/20 Cost:    inf\n",
      "Epoch   10/20 Cost:    inf\n",
      "Epoch   11/20 Cost:    inf\n",
      "Epoch   12/20 Cost:    nan\n",
      "Epoch   13/20 Cost:    nan\n",
      "Epoch   14/20 Cost:    nan\n",
      "Epoch   15/20 Cost:    nan\n",
      "Epoch   16/20 Cost:    nan\n",
      "Epoch   17/20 Cost:    nan\n",
      "Epoch   18/20 Cost:    nan\n",
      "Epoch   19/20 Cost:    nan\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-alliance",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-customer",
   "metadata": {},
   "source": [
    "너무 학습 데이터에 한해 잘 학습해 테스트 데이터에 좋은 성능을 내지 못할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-airline",
   "metadata": {},
   "source": [
    "이것을 방지하는 방법은 크게 세 가지인데\n",
    "1. 더 많은 학습 데이터\n",
    "2. 더 적은 양의 feature\n",
    "3. **Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "compliant-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_regularization(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # l2 norm 계산\n",
    "        l2_reg = 0\n",
    "        for param in model.parameters():\n",
    "#             print('param:', param)\n",
    "#             print('torch.norm(param):', torch.norm(param))\n",
    "            l2_reg += torch.norm(param)\n",
    "            \n",
    "        cost += l2_reg\n",
    "        \n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "environmental-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fossil-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "published-scale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param: Parameter containing:\n",
      "tensor([[ 0.3133, -0.1403,  0.5751]], requires_grad=True)\n",
      "torch.norm(param): tensor(0.6697, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([0.4628], requires_grad=True)\n",
      "torch.norm(param): tensor(0.4628, grad_fn=<CopyBackwards>)\n",
      "Epoch    0/20 Cost: 29477.810547\n",
      "param: Parameter containing:\n",
      "tensor([[3.7605, 3.2587, 4.0359]], requires_grad=True)\n",
      "torch.norm(param): tensor(6.4069, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([34.4702], requires_grad=True)\n",
      "torch.norm(param): tensor(34.4702, grad_fn=<CopyBackwards>)\n",
      "Epoch    1/20 Cost: 18798.513672\n",
      "param: Parameter containing:\n",
      "tensor([[5.6179, 5.0300, 5.9076]], requires_grad=True)\n",
      "torch.norm(param): tensor(9.5792, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([61.6762], requires_grad=True)\n",
      "torch.norm(param): tensor(61.6762, grad_fn=<CopyBackwards>)\n",
      "Epoch    2/20 Cost: 12059.365234\n",
      "param: Parameter containing:\n",
      "tensor([[6.6327, 5.9697, 6.9201]], requires_grad=True)\n",
      "torch.norm(param): tensor(11.2924, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([83.4410], requires_grad=True)\n",
      "torch.norm(param): tensor(83.4410, grad_fn=<CopyBackwards>)\n",
      "Epoch    3/20 Cost: 7773.400391\n",
      "param: Parameter containing:\n",
      "tensor([[7.1925, 6.4617, 7.4687]], requires_grad=True)\n",
      "torch.norm(param): tensor(12.2175, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([100.8528], requires_grad=True)\n",
      "torch.norm(param): tensor(100.8528, grad_fn=<CopyBackwards>)\n",
      "Epoch    4/20 Cost: 5038.264160\n",
      "param: Parameter containing:\n",
      "tensor([[7.5066, 6.7126, 7.7669]], requires_grad=True)\n",
      "torch.norm(param): tensor(12.7174, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([114.7822], requires_grad=True)\n",
      "torch.norm(param): tensor(114.7822, grad_fn=<CopyBackwards>)\n",
      "Epoch    5/20 Cost: 3290.066650\n",
      "param: Parameter containing:\n",
      "tensor([[7.6877, 6.8339, 7.9300]], requires_grad=True)\n",
      "torch.norm(param): tensor(12.9880, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([125.9258], requires_grad=True)\n",
      "torch.norm(param): tensor(125.9258, grad_fn=<CopyBackwards>)\n",
      "Epoch    6/20 Cost: 2171.882324\n",
      "param: Parameter containing:\n",
      "tensor([[7.7969, 6.8856, 8.0201]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.1348, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([134.8406], requires_grad=True)\n",
      "torch.norm(param): tensor(134.8406, grad_fn=<CopyBackwards>)\n",
      "Epoch    7/20 Cost: 1456.434692\n",
      "param: Parameter containing:\n",
      "tensor([[7.8669, 6.9001, 8.0707]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.2150, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([141.9725], requires_grad=True)\n",
      "torch.norm(param): tensor(141.9725, grad_fn=<CopyBackwards>)\n",
      "Epoch    8/20 Cost: 998.598267\n",
      "param: Parameter containing:\n",
      "tensor([[7.9155, 6.8949, 8.0999]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.2591, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([147.6780], requires_grad=True)\n",
      "torch.norm(param): tensor(147.6780, grad_fn=<CopyBackwards>)\n",
      "Epoch    9/20 Cost: 705.595398\n",
      "param: Parameter containing:\n",
      "tensor([[7.9523, 6.8795, 8.1176]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.2839, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([152.2424], requires_grad=True)\n",
      "torch.norm(param): tensor(152.2424, grad_fn=<CopyBackwards>)\n",
      "Epoch   10/20 Cost: 518.073608\n",
      "param: Parameter containing:\n",
      "tensor([[7.9824, 6.8588, 8.1290]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.2983, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([155.8939], requires_grad=True)\n",
      "torch.norm(param): tensor(155.8939, grad_fn=<CopyBackwards>)\n",
      "Epoch   11/20 Cost: 398.057220\n",
      "param: Parameter containing:\n",
      "tensor([[8.0087, 6.8357, 8.1370]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.3070, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([158.8151], requires_grad=True)\n",
      "torch.norm(param): tensor(158.8151, grad_fn=<CopyBackwards>)\n",
      "Epoch   12/20 Cost: 321.242920\n",
      "param: Parameter containing:\n",
      "tensor([[8.0327, 6.8116, 8.1430]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.3127, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([161.1521], requires_grad=True)\n",
      "torch.norm(param): tensor(161.1521, grad_fn=<CopyBackwards>)\n",
      "Epoch   13/20 Cost: 272.078247\n",
      "param: Parameter containing:\n",
      "tensor([[8.0551, 6.7872, 8.1479]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.3168, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([163.0217], requires_grad=True)\n",
      "torch.norm(param): tensor(163.0217, grad_fn=<CopyBackwards>)\n",
      "Epoch   14/20 Cost: 240.609131\n",
      "param: Parameter containing:\n",
      "tensor([[8.0764, 6.7630, 8.1521]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.3201, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([164.5174], requires_grad=True)\n",
      "torch.norm(param): tensor(164.5174, grad_fn=<CopyBackwards>)\n",
      "Epoch   15/20 Cost: 220.465637\n",
      "param: Parameter containing:\n",
      "tensor([[8.0970, 6.7392, 8.1560]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.3229, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([165.7139], requires_grad=True)\n",
      "torch.norm(param): tensor(165.7139, grad_fn=<CopyBackwards>)\n",
      "Epoch   16/20 Cost: 207.570572\n",
      "param: Parameter containing:\n",
      "tensor([[8.1169, 6.7159, 8.1596]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.3254, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([166.6711], requires_grad=True)\n",
      "torch.norm(param): tensor(166.6711, grad_fn=<CopyBackwards>)\n",
      "Epoch   17/20 Cost: 199.314804\n",
      "param: Parameter containing:\n",
      "tensor([[8.1362, 6.6932, 8.1630]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.3278, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([167.4369], requires_grad=True)\n",
      "torch.norm(param): tensor(167.4369, grad_fn=<CopyBackwards>)\n",
      "Epoch   18/20 Cost: 194.028229\n",
      "param: Parameter containing:\n",
      "tensor([[8.1549, 6.6710, 8.1663]], requires_grad=True)\n",
      "torch.norm(param): tensor(13.3301, grad_fn=<CopyBackwards>)\n",
      "param: Parameter containing:\n",
      "tensor([168.0495], requires_grad=True)\n",
      "torch.norm(param): tensor(168.0495, grad_fn=<CopyBackwards>)\n",
      "Epoch   19/20 Cost: 190.642029\n"
     ]
    }
   ],
   "source": [
    "train_with_regularization(model, optimizer, norm_x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-climb",
   "metadata": {},
   "source": [
    "torch.norm은 l2 norm을 구해주는 함수로, 유클리디안 거리를 구하는 식과 같다.  \n",
    "자세한 것은 [링크1](https://light-tree.tistory.com/125)을 참고하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-allah",
   "metadata": {},
   "source": [
    "L2 규제는 기존의 cost에 규제항을 더해준 것으로, 식은 [링크2](https://dailyheumsi.tistory.com/57)를 참고하면 된다.  \n",
    "규제항은 파라미터 각각 L2 norm을 구해서 모두 더한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "egyptian-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.FloatTensor([[ 0.3133, -0.1403,  0.5751]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "existing-murray",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0982, 0.0197, 0.3307]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "resistant-prescription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4486)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "encouraging-dependence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6698)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a ** 2).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "miniature-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6698)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
